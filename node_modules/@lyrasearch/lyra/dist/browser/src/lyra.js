"use strict";
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
Object.defineProperty(exports, "__esModule", { value: true });
exports.defaultTokenizerConfig = exports.load = exports.save = exports.search = exports.remove = exports.insertBatch = exports.insertWithHooks = exports.insert = exports.create = exports.formatNanoseconds = exports.tokenize = void 0;
const en_1 = require("../stemmer/lib/en");
const ERRORS = __importStar(require("./errors"));
const insertion_checker_1 = require("./insertion-checker");
const node_1 = require("./prefix-tree/node");
const trie_1 = require("./prefix-tree/trie");
const tokenizer_1 = require("./tokenizer");
const languages_1 = require("./tokenizer/languages");
const stop_words_1 = require("./tokenizer/stop-words");
const utils_1 = require("./utils");
var tokenizer_2 = require("./tokenizer");
Object.defineProperty(exports, "tokenize", { enumerable: true, get: function () { return tokenizer_2.tokenize; } });
var utils_2 = require("./utils");
Object.defineProperty(exports, "formatNanoseconds", { enumerable: true, get: function () { return utils_2.formatNanoseconds; } });
const SUPPORTED_HOOKS = ["afterInsert"];
function validateHooks(hooks) {
    if (hooks) {
        if (typeof hooks !== "object") {
            throw new Error(ERRORS.INVALID_HOOKS_OBJECT());
        }
        const invalidHooks = Object.keys(hooks).filter(hook => !(0, utils_1.includes)(SUPPORTED_HOOKS, hook));
        if (invalidHooks.length) {
            throw new Error(ERRORS.NON_SUPPORTED_HOOKS(invalidHooks));
        }
    }
}
async function hookRunner(
// eslint-disable-next-line @typescript-eslint/ban-types
funcs, ...args) {
    const hooks = Array.isArray(funcs) ? funcs : [funcs];
    for (let i = 0; i < hooks.length; i++) {
        await hooks[i].apply(this, args);
    }
}
function buildIndex(lyra, schema, prefix = "") {
    for (const prop of Object.keys(schema)) {
        const propType = typeof prop;
        const isNested = typeof schema[prop] === "object";
        if (propType !== "string")
            throw new Error(ERRORS.INVALID_SCHEMA_TYPE(propType));
        const propName = `${prefix}${prop}`;
        if (isNested) {
            buildIndex(lyra, schema[prop], `${propName}.`);
        }
        else {
            lyra.index[propName] = (0, node_1.create)();
        }
    }
}
function recursiveCheckDocSchema(newDoc, schema) {
    for (const key in newDoc) {
        if (!(key in schema)) {
            continue;
        }
        const propType = typeof newDoc[key];
        if (propType === "object") {
            recursiveCheckDocSchema(newDoc[key], schema);
        }
        else if (typeof newDoc[key] !== schema[key]) {
            return false;
        }
    }
    return true;
}
function recursiveTrieInsertion(lyra, doc, id, config, prefix = "", tokenizerConfig, schema = lyra.schema) {
    const { index, nodes, frequencies, tokenOccurrencies } = lyra;
    for (const key of Object.keys(doc)) {
        const isNested = typeof doc[key] === "object";
        const isSchemaNested = typeof schema[key] == "object";
        const propName = `${prefix}${key}`;
        if (isNested && key in schema && isSchemaNested) {
            recursiveTrieInsertion(lyra, doc[key], id, config, propName + ".", tokenizerConfig, schema[key]);
        }
        if (typeof doc[key] === "string" && key in schema && !isSchemaNested) {
            // Use propName here because if doc is a nested object
            // We will get the wrong index
            const requestedTrie = index[propName];
            const tokens = tokenizerConfig.tokenizerFn(doc[key], config.language, false, tokenizerConfig);
            if (!(propName in frequencies)) {
                frequencies[propName] = {};
            }
            if (!(propName in tokenOccurrencies)) {
                tokenOccurrencies[propName] = {};
            }
            if (!(id in frequencies[propName])) {
                frequencies[propName][id] = {};
            }
            for (const token of tokens) {
                let tokenFrequency = 0;
                for (const t of tokens) {
                    if (t === token) {
                        tokenFrequency++;
                    }
                }
                const tf = tokenFrequency / tokens.length;
                frequencies[propName][id][token] = tf;
                if (!(token in tokenOccurrencies[propName])) {
                    tokenOccurrencies[propName][token] = 0;
                }
                tokenOccurrencies[propName][token]++;
                (0, trie_1.insert)(nodes, requestedTrie, token, id);
            }
        }
    }
}
function getIndices(lyra, indices) {
    const knownIndices = Object.keys(lyra.index);
    if (!indices) {
        return knownIndices;
    }
    if (typeof indices === "string") {
        if (indices !== "*") {
            throw new Error(ERRORS.INVALID_PROPERTY(indices, knownIndices));
        }
        return knownIndices;
    }
    for (const index of indices) {
        if (!(0, utils_1.includes)(knownIndices, index)) {
            throw new Error(ERRORS.INVALID_PROPERTY(index, knownIndices));
        }
    }
    return indices;
}
function getDocumentIDsFromSearch(lyra, params) {
    const idx = lyra.index[params.index];
    const searchResult = (0, trie_1.find)(lyra.nodes, idx, {
        term: params.term,
        exact: params.exact,
        tolerance: params.tolerance,
    });
    const ids = new Set();
    for (const key in searchResult) {
        for (const id of searchResult[key]) {
            ids.add(id);
        }
    }
    return Array.from(ids);
}
function assertSupportedLanguage(language) {
    if (!(0, utils_1.includes)(languages_1.SUPPORTED_LANGUAGES, language)) {
        throw new Error(ERRORS.LANGUAGE_NOT_SUPPORTED(language));
    }
}
function assertDocSchema(doc, lyraSchema) {
    if (!recursiveCheckDocSchema(doc, lyraSchema)) {
        throw new Error(ERRORS.INVALID_DOC_SCHEMA(lyraSchema, doc));
    }
}
/**
 * Creates a new database.
 * @param properties Options to initialize the database with.
 * @example
 * // Create a database that stores documents containing 'author' and 'quote' fields.
 * const db = create({
 *   schema: {
 *     author: 'string',
 *     quote: 'string'
 *   },
 *   hooks: {
 *     afterInsert: [afterInsertHook],
 *   }
 * });
 */
function create(properties) {
    var _a, _b, _c;
    const defaultLanguage = (_b = (_a = properties === null || properties === void 0 ? void 0 : properties.defaultLanguage) === null || _a === void 0 ? void 0 : _a.toLowerCase()) !== null && _b !== void 0 ? _b : "english";
    assertSupportedLanguage(defaultLanguage);
    validateHooks(properties.hooks);
    const instance = {
        defaultLanguage,
        schema: properties.schema,
        docs: {},
        nodes: {},
        index: {},
        hooks: properties.hooks || {},
        edge: (_c = properties.edge) !== null && _c !== void 0 ? _c : false,
        tokenizer: defaultTokenizerConfig(defaultLanguage, properties.tokenizer),
        frequencies: {},
        tokenOccurrencies: {},
    };
    buildIndex(instance, properties.schema);
    return instance;
}
exports.create = create;
/**
 * Inserts a document into a database.
 * @param lyra The database to insert document into.
 * @param doc The document to insert.
 * @param config Optional parameter for overriding default configuration.
 * @returns An object containing id of the inserted document.
 * @example
 * const { id } = insert(db, {
 *   quote: 'You miss 100% of the shots you don\'t take',
 *   author: 'Wayne Gretzky - Michael Scott'
 * });
 */
function insert(lyra, doc, config) {
    config = { language: lyra.defaultLanguage, ...config };
    const id = (0, utils_1.uniqueId)();
    assertSupportedLanguage(config.language);
    assertDocSchema(doc, lyra.schema);
    lyra.docs[id] = doc;
    recursiveTrieInsertion(lyra, doc, id, config, undefined, lyra.tokenizer);
    (0, insertion_checker_1.trackInsertion)(lyra);
    return { id };
}
exports.insert = insert;
/**
 * Inserts a document into a database.
 * @param lyra The database to insert document into.
 * @param doc The document to insert.
 * @param config Optional parameter for overriding default configuration.
 * @returns A Promise object containing id of the inserted document.
 * @example
 * const { id } = insert(db, {
 *   quote: 'You miss 100% of the shots you don\'t take',
 *   author: 'Wayne Gretzky - Michael Scott'
 * });
 */
async function insertWithHooks(lyra, doc, config) {
    config = { language: lyra.defaultLanguage, ...config };
    const id = (0, utils_1.uniqueId)();
    assertSupportedLanguage(config.language);
    assertDocSchema(doc, lyra.schema);
    lyra.docs[id] = doc;
    recursiveTrieInsertion(lyra, doc, id, config, undefined, lyra.tokenizer);
    (0, insertion_checker_1.trackInsertion)(lyra);
    if (lyra.hooks.afterInsert) {
        await hookRunner.call(lyra, lyra.hooks.afterInsert, id);
    }
    return { id };
}
exports.insertWithHooks = insertWithHooks;
/**
 * Inserts a large array of documents into a database without blocking the event loop.
 * @param lyra The database to insert document into.
 * @param docs Array of documents to insert.
 * @param config Optional parameter for overriding default configuration.
 * @returns Promise<void>.
 * @example
 * insertBatch(db, [
 *   {
 *     quote: 'You miss 100% of the shots you don\'t take',
 *     author: 'Wayne Gretzky - Michael Scott'
 *   },
 *   {
 *     quote: 'What I cannot createm I do not understand',
 *     author: 'Richard Feynman'
 *   }
 * ]);
 */
async function insertBatch(lyra, docs, config) {
    var _a;
    const batchSize = (_a = config === null || config === void 0 ? void 0 : config.batchSize) !== null && _a !== void 0 ? _a : 1000;
    return new Promise((resolve, reject) => {
        let i = 0;
        async function insertBatch() {
            const batch = docs.slice(i * batchSize, (i + 1) * batchSize);
            i++;
            if (!batch.length) {
                return resolve();
            }
            for (const line of batch) {
                try {
                    await insertWithHooks(lyra, line, config);
                }
                catch (err) {
                    reject(err);
                }
            }
            setTimeout(insertBatch, 0);
        }
        setTimeout(insertBatch, 0);
    });
}
exports.insertBatch = insertBatch;
/**
 * Removes a document from a database.
 * @param lyra The database to remove the document from.
 * @param docID The id of the document to remove.
 * @example
 * const isDeleted = remove(db, 'L1tpqQxc0c2djrSN2a6TJ');
 */
function remove(lyra, docID) {
    if (!lyra.tokenizer) {
        lyra.tokenizer = defaultTokenizerConfig(lyra.defaultLanguage);
    }
    if (!(docID in lyra.docs)) {
        throw new Error(ERRORS.DOC_ID_DOES_NOT_EXISTS(docID));
    }
    const document = lyra.docs[docID] || {};
    const documentKeys = Object.keys(document || {});
    const documentKeysLength = documentKeys.length;
    for (let i = 0; i < documentKeysLength; i++) {
        const key = documentKeys[i];
        const propertyType = lyra.schema[key];
        if (propertyType === "string") {
            const idx = lyra.index[key];
            const tokens = lyra.tokenizer.tokenizerFn(document[key], lyra.defaultLanguage, false, lyra.tokenizer);
            const tokensLength = tokens.length;
            for (let k = 0; k < tokensLength; k++) {
                const token = tokens[k];
                delete lyra.frequencies[key][docID];
                lyra.tokenOccurrencies[key][token]--;
                if (token && (0, trie_1.removeDocumentByWord)(lyra.nodes, idx, token, docID)) {
                    throw new Error(ERRORS.CANT_DELETE_DOCUMENT(docID, key, token));
                }
            }
        }
    }
    lyra.docs[docID] = undefined;
    return true;
}
exports.remove = remove;
/**
 * Searches for documents in a database.
 * @param lyra The database to search.
 * @param params The search query.
 * @param language Optional parameter to override the default language analyzer.
 * @example
 * // Search for documents that contain 'Michael' in the 'author' field.
 * const result = search(db, {
 *   term: 'Michael',
 *   properties: ['author']
 * });
 */
function search(lyra, params, language) {
    var _a, _b;
    if (!language) {
        language = lyra.defaultLanguage;
    }
    if (!lyra.tokenizer) {
        lyra.tokenizer = defaultTokenizerConfig(language);
    }
    const { limit = 10, offset = 0, exact = false, term, properties } = params;
    const tokens = lyra.tokenizer.tokenizerFn(term, language, false, lyra.tokenizer);
    const indices = getIndices(lyra, properties);
    const results = Array.from({
        length: limit,
    });
    const timeStart = (0, utils_1.getNanosecondsTime)();
    // uniqueDocsIDs contains unique document IDs for all the tokens in all the indices.
    const uniqueDocsIDs = new Map();
    // indexMap is an object containing all the indexes considered for the current search,
    // and an array of doc IDs for each token in all the indices.
    //
    // Given the search term "quick brown fox" on the "description" index,
    // indexMap will look like this:
    //
    // {
    //   description: {
    //     quick: [doc1, doc2, doc3],
    //     brown: [doc2, doc4],
    //     fox:   [doc2]
    //   }
    // }
    const indexMap = {};
    // After we create the indexMap, we need to calculate the intersection
    // between all the postings lists for each token.
    // Given the example above, docsIntersection will look like this:
    //
    // {
    //   description: [doc2]
    // }
    //
    // as doc2 is the only document present in all the postings lists for the "description" index.
    const docsIntersection = {};
    for (const index of indices) {
        const tokensMap = {};
        for (const token of tokens) {
            tokensMap[token] = [];
        }
        indexMap[index] = tokensMap;
        docsIntersection[index] = [];
    }
    const N = Object.keys(lyra.docs).length;
    // Now it's time to loop over all the indices and get the documents IDs for every single term
    const indexesLength = indices.length;
    for (let i = 0; i < indexesLength; i++) {
        const index = indices[i];
        if (!(index in lyra.tokenOccurrencies))
            continue;
        const lyraOccurrencies = lyra.tokenOccurrencies[index];
        const lyraFrequencies = lyra.frequencies[index];
        const tokensLength = tokens.length;
        for (let j = 0; j < tokensLength; j++) {
            const term = tokens[j];
            const documentIDs = getDocumentIDsFromSearch(lyra, { ...params, index, term, exact });
            const termOccurrencies = lyraOccurrencies[term];
            const orderedTFIDFList = [];
            // Calculate TF-IDF value for each term, in each document, for each index.
            // Then insert sorted results into orderedTFIDFList.
            const documentIDsLength = documentIDs.length;
            for (let k = 0; k < documentIDsLength; k++) {
                const id = documentIDs[k];
                const idf = Math.log10(N / termOccurrencies);
                const tfIdf = idf * ((_b = (_a = lyraFrequencies === null || lyraFrequencies === void 0 ? void 0 : lyraFrequencies[id]) === null || _a === void 0 ? void 0 : _a[term]) !== null && _b !== void 0 ? _b : 0);
                // @todo: we're now using binary search to insert the element in the right position.
                // Maybe we can switch to sparse array insertion?
                (0, utils_1.insertSortedValue)(orderedTFIDFList, [id, tfIdf], utils_1.sortTokenScorePredicate);
            }
            indexMap[index][term].push(...orderedTFIDFList);
        }
        const docIds = indexMap[index];
        const vals = Object.values(docIds);
        docsIntersection[index] = (0, utils_1.intersectTokenScores)(vals);
        const uniqueDocs = Object.values(docsIntersection[index]);
        const uniqueDocsLength = uniqueDocs.length;
        for (let i = 0; i < uniqueDocsLength; i++) {
            const [id, tfIdfScore] = uniqueDocs[i];
            if (uniqueDocsIDs.has(id)) {
                const prevScore = uniqueDocsIDs.get(id);
                uniqueDocsIDs.set(id, prevScore + tfIdfScore);
            }
            else {
                uniqueDocsIDs.set(id, tfIdfScore);
            }
        }
    }
    // Get unique doc IDs from uniqueDocsIDs map, sorted by value.
    const uniqueDocsArray = Array.from(uniqueDocsIDs.entries()).sort(utils_1.sortTokenScorePredicate);
    const resultIDs = new Set();
    // We already have the list of ALL the document IDs containing the search terms.
    // We loop over them starting from a positional value "offset" and ending at "offset + limit"
    // to provide pagination capabilities to the search.
    for (let i = offset; i < limit + offset; i++) {
        const idAndScore = uniqueDocsArray[i];
        // If there are no more results, just break the loop
        if (typeof idAndScore === "undefined") {
            break;
        }
        const [id, score] = idAndScore;
        if (!resultIDs.has(id)) {
            // We retrieve the full document only AFTER making sure that we really want it.
            // We never retrieve the full document preventively.
            const fullDoc = lyra.docs[id];
            results[i] = { id, score, document: fullDoc };
            resultIDs.add(id);
        }
    }
    const hits = results.filter(Boolean);
    return {
        elapsed: (0, utils_1.getNanosecondsTime)() - timeStart,
        hits,
        count: uniqueDocsIDs.size,
    };
}
exports.search = search;
function save(lyra) {
    return {
        index: lyra.index,
        docs: lyra.docs,
        nodes: lyra.nodes,
        schema: lyra.schema,
        frequencies: lyra.frequencies,
        tokenOccurrencies: lyra.tokenOccurrencies,
    };
}
exports.save = save;
function load(lyra, { index, docs, nodes, schema, frequencies, tokenOccurrencies }) {
    if (!lyra.edge) {
        throw new Error(ERRORS.GETTER_SETTER_WORKS_ON_EDGE_ONLY("load"));
    }
    lyra.index = index;
    lyra.docs = docs;
    lyra.nodes = nodes;
    lyra.schema = schema;
    lyra.frequencies = frequencies;
    lyra.tokenOccurrencies = tokenOccurrencies;
}
exports.load = load;
function defaultTokenizerConfig(language, tokenizerConfig = {}) {
    var _a, _b, _c;
    let defaultStopWords = [];
    let customStopWords = [];
    let defaultStemmingFn;
    let defaultTokenizerFn = tokenizer_1.tokenize;
    // Enable custom tokenizer function
    if (tokenizerConfig === null || tokenizerConfig === void 0 ? void 0 : tokenizerConfig.tokenizerFn) {
        if (typeof tokenizerConfig.tokenizerFn !== "function") {
            throw Error(ERRORS.INVALID_TOKENIZER_FUNCTION());
        }
        defaultTokenizerFn = tokenizerConfig.tokenizerFn;
        // If there's no custom tokenizer, we can proceed setting custom
        // stemming functions and stop-words.
    }
    else {
        // Enable custom stemming function
        if (tokenizerConfig === null || tokenizerConfig === void 0 ? void 0 : tokenizerConfig.stemmingFn) {
            if (typeof tokenizerConfig.stemmingFn !== "function") {
                throw Error(ERRORS.INVALID_STEMMER_FUNCTION_TYPE());
            }
            defaultStemmingFn = tokenizerConfig.stemmingFn;
        }
        else {
            defaultStemmingFn = en_1.stemmer;
        }
        // Enable default stop-words
        if ((0, utils_1.includes)(stop_words_1.availableStopWords, language)) {
            defaultStopWords = (_a = stop_words_1.stopWords[language]) !== null && _a !== void 0 ? _a : [];
        }
        if (tokenizerConfig === null || tokenizerConfig === void 0 ? void 0 : tokenizerConfig.customStopWords) {
            switch (typeof tokenizerConfig.customStopWords) {
                // Execute the custom step-words function.
                // This will pass the default step-words for a given language as a first parameter.
                case "function":
                    customStopWords = tokenizerConfig.customStopWords(defaultStopWords);
                    break;
                // Check if the custom step-words is an array.
                // If it's an object, throw an exception. If the array contains any non-string value, throw an exception.
                case "object":
                    if (!Array.isArray(tokenizerConfig.customStopWords)) {
                        throw Error(ERRORS.CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY());
                    }
                    customStopWords = tokenizerConfig.customStopWords;
                    if (customStopWords.some(x => typeof x !== "string")) {
                        throw Error(ERRORS.CUSTOM_STOP_WORDS_ARRAY_MUST_BE_STRING_ARRAY());
                    }
                    break;
                // By default, throw an exception, as this is a misconfiguration.
                default:
                    throw Error(ERRORS.CUSTOM_STOP_WORDS_MUST_BE_FUNCTION_OR_ARRAY());
            }
        }
    }
    return {
        enableStopWords: (_b = tokenizerConfig === null || tokenizerConfig === void 0 ? void 0 : tokenizerConfig.enableStopWords) !== null && _b !== void 0 ? _b : true,
        enableStemming: (_c = tokenizerConfig === null || tokenizerConfig === void 0 ? void 0 : tokenizerConfig.enableStemming) !== null && _c !== void 0 ? _c : true,
        stemmingFn: defaultStemmingFn,
        customStopWords: customStopWords !== null && customStopWords !== void 0 ? customStopWords : defaultStopWords,
        tokenizerFn: defaultTokenizerFn,
    };
}
exports.defaultTokenizerConfig = defaultTokenizerConfig;
//# sourceMappingURL=lyra.js.map